{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cfe7729",
   "metadata": {},
   "source": [
    "# Medication Adherence Risk â€“ BI & Dashboard Prep Notebook\n",
    "\n",
    "This notebook prepares aggregated datasets for use in **Power BI** / **Tableau** dashboards.\n",
    "\n",
    "It assumes you have already:\n",
    "- Generated synthetic raw data in `data/raw/`\n",
    "- Built the training dataset in `data/processed/training_dataset.csv`\n",
    "- Trained the model and saved it as `models/rf_best_model.joblib`\n",
    "- (Optionally) created `data/processed/outputs/scored_members.csv`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c020d6a",
   "metadata": {},
   "source": [
    "## 1. Setup & Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fb4e720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE_DIR: /Users/ganeshkrishnalakshmisetty/med-adherence-risk/notebooks/data/processed/training_dataset.csv\n",
      "TRAIN_FILE exists: False\n",
      "SCORED_FILE exists: False\n",
      "MODEL_PATH exists: False\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "BASE_DIR = Path.cwd()\n",
    "RAW_DIR = BASE_DIR / \"data\" / \"raw\"\n",
    "PROC_DIR = BASE_DIR / \"data\" / \"processed\"\n",
    "OUT_DIR = PROC_DIR / \"bi\"\n",
    "MODEL_DIR = BASE_DIR / \"models\"\n",
    "\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "TRAIN_FILE = PROC_DIR / \"training_dataset.csv\"\n",
    "SCORED_FILE = PROC_DIR / \"outputs\" / \"scored_members.csv\"\n",
    "MODEL_PATH = MODEL_DIR / \"rf_best_model.joblib\"\n",
    "\n",
    "print(\"BASE_DIR:\", TRAIN_FILE)\n",
    "print(\"TRAIN_FILE exists:\", TRAIN_FILE.exists())\n",
    "print(\"SCORED_FILE exists:\", SCORED_FILE.exists())\n",
    "print(\"MODEL_PATH exists:\", MODEL_PATH.exists())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba5d1e5",
   "metadata": {},
   "source": [
    "## 2. Load or Create Scored Members Dataset\n",
    "\n",
    "We try to load `scored_members.csv` (batch-scored dataset). If it doesn't exist, we:\n",
    "1. Load `training_dataset.csv`\n",
    "2. Load the trained pipeline model\n",
    "3. Compute `prob_non_adherent` and `risk_category` for each row\n",
    "4. Save `scored_members.csv` for future reuse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20580322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scored_members.csv not found; creating it from training data and model...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/ganeshkrishnalakshmisetty/med-adherence-risk/notebooks/data/processed/training_dataset.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     13\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mscored_members.csv not found; creating it from training data and model...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     df_train = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTRAIN_FILE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstart_date\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m     model = joblib.load(MODEL_PATH)\n\u001b[32m     17\u001b[39m     DROP_COLS = [\n\u001b[32m     18\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mlabel_non_adherent\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     19\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mpdc_180d\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     20\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/med-adherence-risk/.venv/lib/python3.14/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/med-adherence-risk/.venv/lib/python3.14/site-packages/pandas/io/parsers/readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/med-adherence-risk/.venv/lib/python3.14/site-packages/pandas/io/parsers/readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/med-adherence-risk/.venv/lib/python3.14/site-packages/pandas/io/parsers/readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/med-adherence-risk/.venv/lib/python3.14/site-packages/pandas/io/common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '/Users/ganeshkrishnalakshmisetty/med-adherence-risk/notebooks/data/processed/training_dataset.csv'"
     ]
    }
   ],
   "source": [
    "def assign_risk_category(prob_nonadherent: float) -> str:\n",
    "    if prob_nonadherent >= 0.8:\n",
    "        return \"HIGH\"\n",
    "    elif prob_nonadherent >= 0.6:\n",
    "        return \"MEDIUM\"\n",
    "    else:\n",
    "        return \"LOW\"\n",
    "\n",
    "if SCORED_FILE.exists():\n",
    "    df_scored = pd.read_csv(SCORED_FILE, parse_dates=[\"start_date\"])\n",
    "    print(\"Loaded existing scored_members.csv:\", df_scored.shape)\n",
    "else:\n",
    "    print(\"scored_members.csv not found; creating it from training data and model...\")\n",
    "    df_train = pd.read_csv(TRAIN_FILE, parse_dates=[\"start_date\"])\n",
    "    model = joblib.load(MODEL_PATH)\n",
    "\n",
    "    DROP_COLS = [\n",
    "        \"label_non_adherent\",\n",
    "        \"pdc_180d\",\n",
    "    ]\n",
    "\n",
    "    X = df_train.drop(columns=DROP_COLS, errors=\"ignore\")\n",
    "    proba = model.predict_proba(X)[:, 1]\n",
    "\n",
    "    df_scored = df_train.copy()\n",
    "    df_scored[\"prob_non_adherent\"] = proba\n",
    "    df_scored[\"risk_category\"] = df_scored[\"prob_non_adherent\"].apply(assign_risk_category)\n",
    "\n",
    "    output_dir = SCORED_FILE.parent\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    df_scored.to_csv(SCORED_FILE, index=False)\n",
    "    print(\"Created and saved scored_members.csv:\", df_scored.shape)\n",
    "\n",
    "df_scored.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d826cd5",
   "metadata": {},
   "source": [
    "## 3. KPI Calculations\n",
    "\n",
    "We compute high-level KPIs you can surface as tiles in Power BI / Tableau:\n",
    "- **% High-Risk Members**\n",
    "- **Average Non-Adherence Risk by Drug Class**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe840d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall % high-risk members\n",
    "total_members = len(df_scored)\n",
    "high_risk_members = (df_scored[\"risk_category\"] == \"HIGH\").sum()\n",
    "pct_high_risk = high_risk_members / total_members if total_members > 0 else 0\n",
    "\n",
    "print(f\"Total members: {total_members}\")\n",
    "print(f\"High-risk members: {high_risk_members}\")\n",
    "print(f\"% High-Risk Members: {pct_high_risk:.2%}\")\n",
    "\n",
    "# Average non-adherence risk by drug class\n",
    "if \"drug_class\" in df_scored.columns:\n",
    "    kpi_drug = (\n",
    "        df_scored\n",
    "        .groupby(\"drug_class\")\n",
    "        [\"prob_non_adherent\"]\n",
    "        .agg([\"count\", \"mean\"])\n",
    "        .rename(columns={\"count\": \"member_count\", \"mean\": \"avg_prob_non_adherent\"})\n",
    "        .sort_values(\"avg_prob_non_adherent\", ascending=False)\n",
    "    )\n",
    "    try:\n",
    "        display(kpi_drug)\n",
    "    except NameError:\n",
    "        print(kpi_drug.head())\n",
    "    kpi_drug.to_csv(OUT_DIR / \"kpi_by_drug_class.csv\")\n",
    "else:\n",
    "    print(\"Column 'drug_class' not found; cannot compute KPI by drug class.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ceff7f",
   "metadata": {},
   "source": [
    "## 4. Geo-Level Aggregations (County / State)\n",
    "\n",
    "We prepare a dataset suitable for **map visuals**:\n",
    "- Number of high-risk members by county/state\n",
    "- High-risk rate\n",
    "- Overlay SDOH metrics (if present in the scored data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a29861e",
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_cols = [\"county\", \"state\"]\n",
    "for col in geo_cols:\n",
    "    if col not in df_scored.columns:\n",
    "        print(f\"Column '{col}' missing; geo aggregations may be incomplete.\")\n",
    "\n",
    "df_geo = df_scored.copy()\n",
    "df_geo[\"is_high_risk\"] = (df_geo[\"risk_category\"] == \"HIGH\").astype(int)\n",
    "\n",
    "group_cols = [c for c in [\"county\", \"state\"] if c in df_geo.columns]\n",
    "if group_cols:\n",
    "    agg_dict = {\n",
    "        \"is_high_risk\": [\"sum\", \"count\"],\n",
    "    }\n",
    "\n",
    "    # Include SDOH metrics if available\n",
    "    sdoh_cols = [\n",
    "        \"pct_uninsured\", \"pct_food_stamp\", \"pct_public_transport\",\n",
    "        \"pct_less_hs_edu\", \"pct_disabled\", \"total_mh_providers\"\n",
    "    ]\n",
    "    for col in sdoh_cols:\n",
    "        if col in df_geo.columns:\n",
    "            agg_dict[col] = \"mean\"\n",
    "\n",
    "    df_geo_agg = (\n",
    "        df_geo\n",
    "        .groupby(group_cols)\n",
    "        .agg(agg_dict)\n",
    "    )\n",
    "\n",
    "    # Flatten MultiIndex columns\n",
    "    df_geo_agg.columns = [\"_\".join([c for c in col if c]) for col in df_geo_agg.columns.values]\n",
    "    df_geo_agg = df_geo_agg.reset_index()\n",
    "\n",
    "    # Rename high-risk cols\n",
    "    df_geo_agg = df_geo_agg.rename(columns={\n",
    "        \"is_high_risk_sum\": \"high_risk_count\",\n",
    "        \"is_high_risk_count\": \"total_members\"\n",
    "    })\n",
    "    df_geo_agg[\"high_risk_rate\"] = (\n",
    "        df_geo_agg[\"high_risk_count\"] / df_geo_agg[\"total_members\"]\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        display(df_geo_agg.head())\n",
    "    except NameError:\n",
    "        print(df_geo_agg.head())\n",
    "    df_geo_agg.to_csv(OUT_DIR / \"geo_high_risk_by_county_state.csv\", index=False)\n",
    "else:\n",
    "    print(\"No geo columns available; skipping geo aggregation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182965fd",
   "metadata": {},
   "source": [
    "## 5. Trend Aggregations Over Time\n",
    "\n",
    "We aggregate risk over time to support **trend charts** in BI tools:\n",
    "- High-risk count by month\n",
    "- Overall member count by month\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0a0134",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"start_date\" in df_scored.columns:\n",
    "    df_trend = df_scored.copy()\n",
    "    df_trend[\"start_month\"] = df_trend[\"start_date\"].dt.to_period(\"M\").dt.to_timestamp()\n",
    "    df_trend[\"is_high_risk\"] = (df_trend[\"risk_category\"] == \"HIGH\").astype(int)\n",
    "\n",
    "    df_trend_agg = (\n",
    "        df_trend\n",
    "        .groupby(\"start_month\")\n",
    "        .agg(\n",
    "            total_members=(\"member_id\", \"count\"),\n",
    "            high_risk_count=(\"is_high_risk\", \"sum\"),\n",
    "            avg_prob_non_adherent=(\"prob_non_adherent\", \"mean\")\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    df_trend_agg[\"high_risk_rate\"] = (\n",
    "        df_trend_agg[\"high_risk_count\"] / df_trend_agg[\"total_members\"]\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        display(df_trend_agg.head())\n",
    "    except NameError:\n",
    "        print(df_trend_agg.head())\n",
    "    df_trend_agg.to_csv(OUT_DIR / \"trend_high_risk_by_month.csv\", index=False)\n",
    "else:\n",
    "    print(\"Column 'start_date' missing; cannot compute time trends.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460fd017",
   "metadata": {},
   "source": [
    "## 6. High-Risk by Top Pharmacies / Providers\n",
    "\n",
    "We join scored members with **primary pharmacy** and **primary provider**\n",
    "derived from `pharmacy_claims.csv`, and then aggregate:\n",
    "- High-risk count by pharmacy\n",
    "- High-risk count by ordering provider\n",
    "\n",
    "We define a member's primary pharmacy/provider as the one with the highest\n",
    "number of claims in their history.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ad921d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pharm_file = RAW_DIR / \"pharmacy_claims.csv\"\n",
    "if pharm_file.exists():\n",
    "    df_claims = pd.read_csv(pharm_file, parse_dates=[\"fill_date\"])\n",
    "\n",
    "    # Compute primary pharmacy per member\n",
    "    grp_ph = (\n",
    "        df_claims\n",
    "        .groupby([\"member_id\", \"pharmacy_id\"])\n",
    "        .size()\n",
    "        .reset_index(name=\"claim_count\")\n",
    "    )\n",
    "    # For each member, take pharmacy_id with max claim_count\n",
    "    idx_max_ph = grp_ph.groupby(\"member_id\")[\"claim_count\"].idxmax()\n",
    "    df_primary_pharm = grp_ph.loc[idx_max_ph, [\"member_id\", \"pharmacy_id\"]]\n",
    "\n",
    "    # Compute primary provider per member\n",
    "    grp_pr = (\n",
    "        df_claims\n",
    "        .groupby([\"member_id\", \"ordering_provider_id\"])\n",
    "        .size()\n",
    "        .reset_index(name=\"claim_count\")\n",
    "    )\n",
    "    idx_max_pr = grp_pr.groupby(\"member_id\")[\"claim_count\"].idxmax()\n",
    "    df_primary_prov = grp_pr.loc[idx_max_pr, [\"member_id\", \"ordering_provider_id\"]]\n",
    "\n",
    "    # Join with scored members\n",
    "    df_scored_ph = df_scored.merge(df_primary_pharm, on=\"member_id\", how=\"left\")\n",
    "    df_scored_ph_pr = df_scored_ph.merge(df_primary_prov, on=\"member_id\", how=\"left\")\n",
    "    df_scored_ph_pr[\"is_high_risk\"] = (df_scored_ph_pr[\"risk_category\"] == \"HIGH\").astype(int)\n",
    "\n",
    "    # Top 10 pharmacies by member volume\n",
    "    df_pharm_agg = (\n",
    "        df_scored_ph_pr\n",
    "        .groupby(\"pharmacy_id\")\n",
    "        .agg(\n",
    "            total_members=(\"member_id\", \"count\"),\n",
    "            high_risk_count=(\"is_high_risk\", \"sum\"),\n",
    "            avg_prob_non_adherent=(\"prob_non_adherent\", \"mean\")\n",
    "        )\n",
    "        .reset_index()\n",
    "        .sort_values(\"total_members\", ascending=False)\n",
    "    )\n",
    "    df_pharm_agg[\"high_risk_rate\"] = (\n",
    "        df_pharm_agg[\"high_risk_count\"] / df_pharm_agg[\"total_members\"]\n",
    "    )\n",
    "\n",
    "    df_top_pharm = df_pharm_agg.head(10)\n",
    "    try:\n",
    "        display(df_top_pharm)\n",
    "    except NameError:\n",
    "        print(df_top_pharm.head())\n",
    "    df_top_pharm.to_csv(OUT_DIR / \"high_risk_by_top_pharmacies.csv\", index=False)\n",
    "\n",
    "    # Top 10 providers by member volume\n",
    "    df_prov_agg = (\n",
    "        df_scored_ph_pr\n",
    "        .groupby(\"ordering_provider_id\")\n",
    "        .agg(\n",
    "            total_members=(\"member_id\", \"count\"),\n",
    "            high_risk_count=(\"is_high_risk\", \"sum\"),\n",
    "            avg_prob_non_adherent=(\"prob_non_adherent\", \"mean\")\n",
    "        )\n",
    "        .reset_index()\n",
    "        .sort_values(\"total_members\", ascending=False)\n",
    "    )\n",
    "    df_prov_agg[\"high_risk_rate\"] = (\n",
    "        df_prov_agg[\"high_risk_count\"] / df_prov_agg[\"total_members\"]\n",
    "    )\n",
    "\n",
    "    df_top_prov = df_prov_agg.head(10)\n",
    "    try:\n",
    "        display(df_top_prov)\n",
    "    except NameError:\n",
    "        print(df_top_prov.head())\n",
    "    df_top_prov.to_csv(OUT_DIR / \"high_risk_by_top_providers.csv\", index=False)\n",
    "else:\n",
    "    print(\"pharmacy_claims.csv not found; skipping top pharmacies/providers aggregations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124bda82",
   "metadata": {},
   "source": [
    "## 7. Summary\n",
    "\n",
    "This notebook generated aggregated datasets under `data/processed/bi/` that can be used in Power BI / Tableau:\n",
    "\n",
    "- `kpi_by_drug_class.csv`\n",
    "- `geo_high_risk_by_county_state.csv`\n",
    "- `trend_high_risk_by_month.csv`\n",
    "- `high_risk_by_top_pharmacies.csv`\n",
    "- `high_risk_by_top_providers.csv`\n",
    "\n",
    "In BI, you can build:\n",
    "- KPI tiles: overall % high-risk, avg risk by drug class\n",
    "- Maps: high-risk count & rate by county/state, with SDOH overlays\n",
    "- Trend charts: high-risk count & rate over time, by month\n",
    "- Bar charts: high-risk rate by top pharmacies and providers\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
